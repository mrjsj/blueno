{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#welcome-to-blueno","title":"Welcome to blueno!","text":"<p>blueno is your friendly neighborhood Python library for building data pipelines the bueno way! </p>"},{"location":"#what-is-blueno","title":"What is blueno? \ud83e\udd14","text":"<p>blueno is a powerful yet easy-to-use Python library that helps you build and run data pipelines. Think of it as your Swiss Army knife for data engineering tasks! It's built on top of amazing tools like Polars for fast data processing and delta-rs for reliable data storage.</p>"},{"location":"#why-blueno","title":"Why blueno?","text":"<ul> <li>Local Development First: Write and test your data pipelines on your laptop</li> <li>ETL Made Easy: Simple tools for extracting, transforming, and loading data</li> <li>Blueprints: Write clean, declarative data pipelines that are easy to understand</li> <li>Platform Agnostic: Works with your choice of storage and compute</li> </ul>"},{"location":"#inspired-by-the-best","title":"Inspired By The Best","text":"<p>Our blueprint system is inspired by great tools like:</p> <ul> <li>dbt models</li> <li>SQLMesh models</li> <li>Dagster's software defined assets</li> </ul>"},{"location":"#ready-to-start","title":"Ready to Start? \ud83d\ude80","text":"<p>Check out our Quick Start Guide to begin your journey with blueno!</p>"},{"location":"api/blueprint/","title":"Blueprint","text":""},{"location":"api/blueprint/#blueprint_1","title":"Blueprint","text":"<p>Create a decorator for the Blueprint.</p> <p>A blueprint is a function that takes any number of blueprints (or zero) and returns a dataframe. In addition, blueprint-information registered to know how to write the dataframe to a target table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the blueprint. If not provided, the name of the function will be used. The name must be unique across all blueprints.</p> <code>None</code> <code>table_uri</code> <code>Optional[str]</code> <p>The URI of the target table. If not provided, the blueprint will not be stored as a table.</p> <code>None</code> <code>schema</code> <code>Optional[Schema]</code> <p>The schema of the output dataframe. If provided, transformation function will be validated against this schema.</p> <code>None</code> <code>primary_keys</code> <code>Optional[List[str]]</code> <p>The primary keys of the target table. Is required for <code>upsert</code> <code>naive_upsert</code> and <code>scd2_by_column</code> write_mode.</p> <code>None</code> <code>partition_by</code> <code>Optional[List[str]]</code> <p>The columns to partition the of the target table by.</p> <code>None</code> <code>incremental_column</code> <code>Optional[str]</code> <p>The incremental column for the target table. Is required for <code>incremental</code> and <code>safe_append</code> write mode.</p> <code>None</code> <code>scd2_column</code> <code>Optional[str]</code> <p>The name of the sequence column used for SCD2. Is required for <code>scd2_by_column</code> and <code>scd2_by_time</code> write mode.</p> <code>None</code> <code>write_mode</code> <code>str</code> <p>The write method to use. Defaults to <code>overwrite</code>. Options are: <code>append</code>, <code>safe_append</code>, <code>overwrite</code>, <code>upsert</code>, <code>naive_upsert</code> <code>incremental</code>, <code>replace_range</code>, and <code>scd2_by_column</code>. - <code>append</code>: Appends all records from the source dataframe into the target. - <code>safe_append</code>: Filters the source dataframe on existing primary keys and <code>incremental_column</code> value and then appends. - <code>upsert</code>: Updates the target table if there are any changes on existing primary keys, and inserts records with primary keys which doesn't exist in the target table. - <code>naive_upsert</code>: Same as upsert, but skips checking for changes and performs a blind update. - <code>incremental</code>: Filters the source dataframe on the max value of the <code>incremental_column</code> value and then appends. - <code>replace_range</code>: Overwrites a range the target table between the minimum and the maximum value of the <code>incremental_column</code> value in the source dataframe. - <code>scd2_by_column</code>: Performs a SCD2 Type upsert where the validity periods created by the <code>scd2_column</code> column.</p> <code>'overwrite'</code> <code>format</code> <code>str</code> <p>The format to use. Defaults to <code>delta</code>. Options are: <code>delta</code>, <code>parquet</code>, and <code>dataframe</code>. If <code>dataframe</code> is used, the blueprint will be stored in memory and not written to a target table.</p> <code>'dataframe'</code> <code>tags</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary of tags to apply to the blueprint. This can be used to group related blueprints by tag, and can be used to run a subset of blueprints based on tags.</p> <code>None</code> <code>post_transforms</code> <code>Optional[List[str]]</code> <p>Optional list of post-transformation functions to apply after the main transformation. Options are: <code>deduplicate</code>, <code>add_audit_columns</code>, <code>add_identity_column</code>. These functions will be applied in the order they are provided.</p> <code>None</code> <code>deduplication_order_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to use for deduplication when <code>post_transforms</code> includes <code>deduplicate</code>.</p> <code>None</code> <code>priority</code> <code>int</code> <p>Determines the execution order among activities ready to run. Higher values indicate higher scheduling preference, but dependencies and concurrency limits are still respected.</p> <code>100</code> <code>max_concurrency</code> <code>Optional[int]</code> <p>Maximum number of parallel executions allowed when this job is running. When set, limits the global concurrency while this blueprint is running. This is useful for blueprints with high CPU or memory requirements. For example, setting max_concurrency=1 ensures this job runs serially, while still allowing other jobs to run in parallel. Higher priority jobs will be scheduled first when concurrent limits are reached.</p> <code>None</code> <code>freshness</code> <code>Optional[timedelta]</code> <p>Optional freshness threshold for the blueprint. Only applicable if the format is <code>delta</code>. If set, the blueprint will only be processed if the delta table's last modification time is older than the freshness threshold. E.g., setting this to <code>timedelta(hours=1)</code> will ensure this blueprint is only run at most once an hour.</p> <code>None</code> <code>schedule</code> <code>Optional[str]</code> <p>Optional cron style for schedule. If blueno runs at a time in the intervals of the schedule, it will be run. Otherwise it will be skipped. If not provided, blueprint will always be executed. Still respects freshness. For instance <code>* * * * 1-5</code> will run Monday through Friday.</p> <code>None</code> <code>maintenance_schedule</code> <code>Optional[str]</code> <p>Optional cron style for table maintenance. Table maintenance compacts and vacuums the delta table. If not provided, no maintenance will be executed. Maintenance will only run once during a cron interval. Setting this value to <code>* 0-8 * * 6</code> will run maintenance on the first run on Saturdays between 0 and 8.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the blueprint. This is used when extending the blueprint with custom attributes or methods.</p> <code>{}</code> <p>Simple example <pre><code>from blueno import Blueprint, DataFrameType\n\n\n@Blueprint.register(\n    table_uri=\"/path/to/stage/customer\",\n    format=\"delta\",\n    primary_keys=[\"customer_id\"],\n    write_mode=\"overwrite\",\n)\ndef stage_customer(self: Blueprint, bronze_customer: DataFrameType) -&gt; DataFrameType:\n    # Deduplicate customers\n    df = bronze_customer.unique(subset=self.primary_keys)\n\n    return df\n</code></pre></p> <p>Full example <pre><code>from blueno import Blueprint, DataFrameType\nfrom datetime import timedelta\n\n\n@Blueprint.register(\n    name=\"gold_customer\",\n    table_uri=\"/path/to/gold/customer\",\n    primary_keys=[\"customer_id\", \"site_id\"],\n    partition_by=[\"year\", \"month\", \"day\"],\n    incremental_column=\"order_timestamp\",\n    scd2_column=\"modified_timestamp\",\n    write_mode=\"upsert\",\n    format=\"delta\",\n    tags={\n        \"owner\": \"Alice Wonderlands\",\n        \"project\": \"customer_360\",\n        \"pii\": \"true\",\n        \"business_unit\": \"finance\",\n    },\n    post_transforms=[\n        \"deduplicate\",\n        \"add_audit_columns\",\n        \"apply_scd2_by_column\",\n    ],\n    deduplication_order_columns=[\"modified_timestamp\"],\n    priority=110,\n    max_concurrency=2,\n    # Will only run if last materialization is more than 1 day ago.\n    freshness=timedelta(days=1),\n    # Runs if blueno runs on Mondays through Fridays - but only once a day at maximum due to `freshness` setting.\n    schedule=\"* * * * 1-5\",\n    # Runs maintenance if blueno is run between 22 and 23 on Saturdays.\n    maintenance_schedule=\"* 22 * 6 *\",\n)\ndef gold_customer(self: Blueprint, silver_customer: DataFrameType) -&gt; DataFrameType:\n    # Some advanced business logic\n    df = silver_customer.with_columns(...)\n\n    return df\n</code></pre></p>"},{"location":"api/blueprint/#task","title":"Task","text":"<p>Create a definition for task.</p> <p>A task can be anything and doesn't need to provide an output.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the blueprint. If not provided, the name of the function will be used. The name must be unique across all jobs.</p> <code>None</code> <code>tags</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary of tags to apply to the blueprint. This can be used to group related jobs by tag, and can be used to run a subset of jobs based on tags.</p> <code>None</code> <code>priority</code> <code>int</code> <p>Determines the execution order among activities ready to run. Higher values indicate higher scheduling preference, but dependencies and concurrency limits are still respected.</p> <code>100</code> <p>Simple example</p> <p>Creates a task for the <code>notify_end_task</code>, which is depends on a gold blueprint. <pre><code>from blueno import Blueprint, Task\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@Task.register()\ndef notify_end_task(gold_metrics: Blueprint) -&gt; None:\n    logger.info(\"Gold metrics ran successfully\")\n\n    # Send message on Slack\n</code></pre></p>"},{"location":"api/etl/","title":"ETL","text":""},{"location":"api/etl/#extract","title":"Extract","text":""},{"location":"api/etl/#blueno.etl.read.delta.read_delta","title":"read_delta","text":"<pre><code>read_delta(\n    table_uri: str, eager: bool = False\n) -&gt; DataFrameType\n</code></pre> <p>Reads a Delta table from the specified abfss URI. Automatically handles the authentication with OneLake.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>str</code> <p>The abfss URI of the Delta table to read.</p> required <code>eager</code> <code>bool</code> <p>If True, reads the table eagerly; otherwise, returns a lazy frame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataFrameType</code> <code>DataFrameType</code> <p>The data from the Delta table.</p> Example <pre><code>from blueno.etl import read_delta\n\nworkspace_id = \"12345678-1234-1234-1234-123456789012\"\nlakehouse_id = \"beefbeef-beef-beef-beef-beefbeefbeef\"\ntable_name = \"my-delta-table\"\ntable_uri = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{table_name}\"\n\ndf = read_delta(table_uri, eager=True)\nlazy_df = read_delta(table_uri, eager=False)\n</code></pre>"},{"location":"api/etl/#blueno.etl.read.parquet.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(\n    table_uri: str, eager: bool = False\n) -&gt; DataFrameType\n</code></pre> <p>Reads a Parquet file from the specified abfss URI. Automatically handles the authentication with OneLake.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>str</code> <p>The abfss URI of the Parquet file to read. Supports globbing.</p> required <code>eager</code> <code>bool</code> <p>If True, reads the file eagerly; otherwise, returns a lazy frame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The data from the Parquet file.</p> Example <p>Reading a single file <pre><code>from blueno.etl import read_parquet\n\nworkspace_id = \"12345678-1234-1234-1234-123456789012\"\nlakehouse_id = \"beefbeef-beef-beef-beef-beefbeefbeef\"\n\nfile_path = \"my-parquet-file.parquet\"\nfolder_uri = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files/\"\n\ndf = read_parquet(folder_uri + file_path, eager=True)\n</code></pre></p> <p>Reading all Parquet files in a folder <pre><code>from blueno.etl import read_parquet\n\nworkspace_id = \"12345678-1234-1234-1234-123456789012\"\nlakehouse_id = \"beefbeef-beef-beef-beef-beefbeefbeef\"\n\nfolder_uri = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files/\"\nglob_df = read_parquet(folder_uri + \"**/*.parquet\", eager=True)\n</code></pre></p>"},{"location":"api/etl/#transforms","title":"Transforms","text":""},{"location":"api/etl/#blueno.etl.transform.transforms.add_audit_columns","title":"add_audit_columns","text":"<pre><code>add_audit_columns(\n    df: DataFrameType, audit_columns: list[Column]\n) -&gt; DataFrameType\n</code></pre> <p>Adds audit columns to the given DataFrame or LazyFrame based on the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameType</code> <p>The DataFrame or LazyFrame to which audit columns will be added.</p> required <code>audit_columns</code> <code>list[Column]</code> <p>A list of audit columns to put on the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The DataFrame or LazyFrame with the added audit columns.</p> Example <pre><code>from blueno.etl import add_audit_columns, Column\nimport polars as pl\nfrom datetime import datetime, timezone\n\naudit_columns = [\n    Column(\"created_at\", pl.lit(datetime.now(timezone.utc)).cast(pl.Datetime(\"us\", \"UTC\")))\n]\ndf = pl.DataFrame({\"data\": [1, 2, 3]})\nupdated_df = add_audit_columns(df, audit_columns)\n</code></pre>"},{"location":"api/etl/#blueno.etl.transform.transforms.deduplicate","title":"deduplicate","text":"<pre><code>deduplicate(\n    df: DataFrameType,\n    key_columns: Optional[List[str]] = None,\n    deduplication_order_columns: Optional[List[str]] = None,\n    deduplication_order_descending: bool = True,\n) -&gt; DataFrameType\n</code></pre> <p>Removes duplicate rows from the DataFrame based on primary key columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameType</code> <p>The DataFrame or LazyFrame from which duplicates will be removed.</p> required <code>key_columns</code> <code>Optional[List[str]]</code> <p>The columns to use as primary keys for deduplication.</p> <code>None</code> <code>deduplication_order_columns</code> <code>Optional[List[str]]</code> <p>The columns to determine the order of rows for deduplication.</p> <code>None</code> <code>deduplication_order_descending</code> <code>bool</code> <p>Whether to sort the deduplication order in descending order.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The DataFrame or LazyFrame with duplicates removed.</p> Example <pre><code>import polars as pl\nfrom blueno.etl import deduplicate\n\ndf = pl.DataFrame({\"id\": [1, 2, 2, 3], \"value\": [\"a\", \"b\", \"b\", \"c\"]})\ndeduped_df = deduplicate(df, key_columns=[\"id\"])\n</code></pre>"},{"location":"api/etl/#blueno.etl.transform.transforms.normalize_column_names","title":"normalize_column_names","text":"<pre><code>normalize_column_names(\n    df: DataFrameType,\n    normalization_strategy: Callable[[str], str],\n) -&gt; DataFrameType\n</code></pre> <p>Normalizes the column names of the DataFrame using a provided normalization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameType</code> <p>The DataFrame or LazyFrame whose column names will be normalized.</p> required <code>normalization_strategy</code> <code>Callable[[str], str]</code> <p>A callable which takes a string and returns a modified string.</p> required <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The DataFrame or LazyFrame with normalized column names.</p> Example <pre><code>import polars as pl\nfrom blueno.etl import normalize_column_names\n\n\ndef my_strategy(old_column_name: str) -&gt; str:\n    new_name = old_column_name.replace(\" \", \"_\").lower()\n    return new_name\n\n\ndf = pl.DataFrame({\"First Name\": [1, 2], \"Last Name\": [3, 4]})\nnormalized_df = normalize_column_names(df, my_strategy)\n</code></pre>"},{"location":"api/etl/#blueno.etl.transform.transforms.reorder_columns_by_suffix","title":"reorder_columns_by_suffix","text":"<pre><code>reorder_columns_by_suffix(\n    df: DataFrameType,\n    suffix_order: List[str],\n    sort_alphabetically_within_group: bool = True,\n) -&gt; DataFrameType\n</code></pre> <p>Reorders DataFrame columns based on their suffixes according to the provided order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameType</code> <p>The DataFrame or LazyFrame whose columns will be reordered.</p> required <code>suffix_order</code> <code>List[str]</code> <p>List of suffixes in the desired order.</p> required <code>sort_alphabetically_within_group</code> <code>bool</code> <p>Whether to sort columns alphabetically within each suffix group.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The DataFrame or LazyFrame with reordered columns.</p> Example <pre><code>import polars as pl\nfrom blueno.etl import reorder_columns_by_suffix\n\n\ndf = pl.DataFrame(\n    {\n        \"name_key\": [\"a\", \"b\"],\n        \"age_key\": [1, 2],\n        \"name_value\": [\"x\", \"y\"],\n        \"age_value\": [10, 20],\n    }\n)\nreordered_df = reorder_columns_by_suffix(df, suffix_order=[\"_pk\", \"_fk\"])\n</code></pre>"},{"location":"api/etl/#blueno.etl.transform.transforms.reorder_columns_by_prefix","title":"reorder_columns_by_prefix","text":"<pre><code>reorder_columns_by_prefix(\n    df: DataFrameType,\n    prefix_order: List[str],\n    sort_alphabetically_within_group: bool = True,\n) -&gt; DataFrameType\n</code></pre> <p>Reorders DataFrame columns based on their prefixes according to the provided order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameType</code> <p>The DataFrame or LazyFrame whose columns will be reordered.</p> required <code>prefix_order</code> <code>List[str]</code> <p>List of prefixes in the desired order.</p> required <code>sort_alphabetically_within_group</code> <code>bool</code> <p>Whether to sort columns alphabetically within each prefix group.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrameType</code> <p>The DataFrame or LazyFrame with reordered columns.</p> Example <pre><code>import polars as pl\nfrom blueno.etl import reorder_columns_by_prefix\n\ndf = pl.DataFrame(\n    {\n        \"dim_name\": [\"a\", \"b\"],\n        \"dim_age\": [1, 2],\n        \"fact_sales\": [100, 200],\n        \"fact_quantity\": [5, 10],\n    }\n)\nreordered_df = reorder_columns_by_prefix(df, prefix_order=[\"pk_\", \"fk_\"])\n</code></pre>"},{"location":"api/etl/#blueno.etl.transform.transforms.apply_scd_type_2","title":"apply_scd_type_2","text":"<pre><code>apply_scd_type_2(\n    source_df: DataFrameType,\n    target_df: DataFrameType,\n    primary_key_columns: List[str],\n    valid_from_column: str,\n    valid_to_column: str,\n) -&gt; DataFrameType\n</code></pre> <p>Applies Slowly Changing Dimension (SCD) Type 2 logic to merge source and target DataFrames.</p> <p>SCD Type 2 maintains historical records by creating new rows for changed data while preserving the history through valid_from and valid_to dates.</p> <p>The result maintains the full history of changes while ensuring proper date ranges for overlapping records.</p> <p>Parameters:</p> Name Type Description Default <code>source_df</code> <code>DataFrameType</code> <p>The new/source DataFrame containing updated records.</p> required <code>target_df</code> <code>DataFrameType</code> <p>The existing/target DataFrame containing current records.</p> required <code>primary_key_columns</code> <code>List[str]</code> <p>Column(s) that uniquely identify each entity.</p> required <code>valid_from_column</code> <code>str</code> <p>Column name containing the validity start date.</p> required <code>valid_to_column</code> <code>str</code> <p>Column name containing the validity end date.</p> required <p>Returns:</p> Type Description <code>DataFrameType</code> <p>A DataFrame containing both current and historical records with updated validity periods.</p> Example <pre><code>import polars as pl\nfrom blueno.etl import apply_scd_type_2\nfrom datetime import datetime\n\n# Create sample source and target dataframes\nsource_df = pl.DataFrame({\n    \"customer_id\": [1, 2],\n    \"name\": [\"John Updated\", \"Jane Updated\"],\n    \"valid_from\": [datetime(2024, 1, 1), datetime(2024, 1, 1)],\n})\n\ntarget_df = pl.DataFrame({\n    \"customer_id\": [1, 2],\n    \"name\": [\"John\", \"Jane\"],\n    \"valid_from\": [datetime(2023, 1, 1), datetime(2023, 1, 1)],\n    \"valid_to\": [None, None]\n})\n\n# Apply SCD Type 2\nresult_df = apply_scd_type_2(\n    source_df=source_df,\n    target_df=target_df,\n    primary_key_columns=\"customer_id\",\n    valid_from_column=\"valid_from\",\n    valid_to_column=\"valid_to\"\n)\n\nprint(result_df.sort(\"customer_id\", \"valid_from\"))\n\n\"\"\"\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name         \u2506 valid_from          \u2506 valid_to            \u2502\n\u2502 ---         \u2506 ---          \u2506 ---                 \u2506 ---                 \u2502\n\u2502 i64         \u2506 str          \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 John         \u2506 2023-01-01 00:00:00 \u2506 2024-01-01 00:00:00 \u2502\n\u2502 1           \u2506 John Updated \u2506 2024-01-01 00:00:00 \u2506 null                \u2502\n\u2502 2           \u2506 Jane         \u2506 2023-01-01 00:00:00 \u2506 2024-01-01 00:00:00 \u2502\n\u2502 2           \u2506 Jane Updated \u2506 2024-01-01 00:00:00 \u2506 null                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n</code></pre> Notes <ul> <li>The function handles overlapping date ranges by adjusting valid_to dates</li> <li>NULL in valid_to indicates a currently active record</li> <li>Records in source_df will create new versions if they differ from target_df</li> <li>Historical records are preserved with appropriate valid_to dates</li> </ul>"},{"location":"api/etl/#blueno.etl.transform.transforms.apply_soft_delete_flag","title":"apply_soft_delete_flag","text":"<pre><code>apply_soft_delete_flag(\n    source_df: DataFrameType,\n    target_df: DataFrameType,\n    primary_key_columns: List[str],\n    soft_delete_column: str,\n) -&gt; DataFrameType\n</code></pre> <p>Marks records as deleted which exists in the target dataframe but not in the source dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>source_df</code> <code>DataFrameType</code> <p>The new/source DataFrame containing updated records.</p> required <code>target_df</code> <code>DataFrameType</code> <p>The existing/target DataFrame containing current records.</p> required <code>primary_key_columns</code> <code>List[str]</code> <p>Column(s) that uniquely identify each record.</p> required <code>soft_delete_column</code> <code>str</code> <p>Column name for the soft delete column to be added.</p> required <p>Returns:</p> Type Description <code>DataFrameType</code> <p>A DataFrame containing the UNION of target dataframe rows marked as soft deleted and the source dataframe rows marked as NOT soft deleted.</p>"},{"location":"api/etl/#load","title":"Load","text":""},{"location":"api/etl/#delta","title":"Delta","text":""},{"location":"api/etl/#blueno.etl.load.delta.upsert","title":"upsert","text":"<pre><code>upsert(\n    table_or_uri: Union[str, DeltaTable],\n    df: DataFrameType,\n    key_columns: List[str],\n    update_exclusion_columns: Optional[List[str]] = None,\n    predicate_exclusion_columns: Optional[List[str]] = None,\n) -&gt; Dict[str, str] | None\n</code></pre> <p>Updates existing records and inserts new records into a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>Union[str, DeltaTable]</code> <p>Path to the Delta table or a DeltaTable instance</p> required <code>df</code> <code>DataFrameType</code> <p>Data to upsert as a Polars DataFrame or LazyFrame</p> required <code>key_columns</code> <code>List[str]</code> <p>Column(s) that uniquely identify each record</p> required <code>update_exclusion_columns</code> <code>Optional[List[str]]</code> <p>Columns that should never be updated (e.g., created_at)</p> <code>None</code> <code>predicate_exclusion_columns</code> <code>Optional[List[str]]</code> <p>Columns to ignore when checking for changes</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str] | None</code> <p>Dict containing merge operation statistics</p> Example <pre><code>from blueno.etl import upsert\nimport polars as pl\n\n# Create sample data\ndata = pl.DataFrame({\"id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n\n# Upsert data using 'id' as the key column\nupsert(\"path/to/upsert_delta_table\", data, key_columns=[\"id\"])\n</code></pre>"},{"location":"api/etl/#blueno.etl.load.delta.overwrite","title":"overwrite","text":"<pre><code>overwrite(\n    table_or_uri: str | DeltaTable, df: DataFrameType\n) -&gt; None\n</code></pre> <p>Replaces all data in a Delta table with new data.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>Path to the Delta table or a DeltaTable instance</p> required <code>df</code> <code>DataFrameType</code> <p>Data to write as a Polars DataFrame or LazyFrame</p> required Example <pre><code>from blueno.etl import overwrite\nimport polars as pl\n\n# Create sample data\ndata = pl.DataFrame({\"id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n\n# Replace entire table with new data\noverwrite(\"path/to/overwrite_delta_table\", data)\n</code></pre>"},{"location":"api/etl/#blueno.etl.load.delta.replace_range","title":"replace_range","text":"<pre><code>replace_range(\n    table_or_uri: str | DeltaTable,\n    df: DataFrameType,\n    range_column: str,\n) -&gt; None\n</code></pre> <p>Replaces data within a specific range in the Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>Path to the Delta table or a DeltaTable instance</p> required <code>df</code> <code>DataFrameType</code> <p>Data to write as a Polars DataFrame or LazyFrame</p> required <code>range_column</code> <code>str</code> <p>Column used to define the range. Records in the table with values between the min and max of this column in df will be replaced</p> required Example <pre><code>from blueno.etl import replace_range\nimport polars as pl\n\n# Create sample data for dates 2024-01-01 to 2024-01-31\ndata = pl.DataFrame({\"date\": [\"2024-01-01\", \"2024-01-31\"], \"value\": [100, 200]})\n\n# Replace all records between Jan 1-31\nreplace_range(\"path/to/replace_range_delta_table\", data, range_column=\"date\")\n</code></pre>"},{"location":"api/etl/#blueno.etl.load.delta.append","title":"append","text":"<pre><code>append(\n    table_or_uri: str | DeltaTable, df: DataFrameType\n) -&gt; None\n</code></pre> <p>Appends the provided dataframe to the Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>The URI of the target Delta table</p> required <code>df</code> <code>DataFrameType</code> <p>The dataframe to append to the Delta table</p> required Example <pre><code>from blueno.etl import append\nimport polars as pl\n\n# Create sample data\ndata = pl.DataFrame({\"id\": [1, 2], \"name\": [\"Alice\", \"Bob\"]})\n\n# Append data to table\nappend(\"path/to/append_delta_table\", data)\n</code></pre>"},{"location":"api/etl/#blueno.etl.load.delta.incremental","title":"incremental","text":"<pre><code>incremental(\n    table_or_uri: str | DeltaTable,\n    df: DataFrameType,\n    incremental_column: str,\n) -&gt; None\n</code></pre> <p>Appends only new records based on an incremental column value.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>Path to the Delta table or a DeltaTable instance</p> required <code>df</code> <code>DataFrameType</code> <p>Data to append as a Polars DataFrame or LazyFrame</p> required <code>incremental_column</code> <code>str</code> <p>Column used to identify new records. Only records where this column's value is greater than the maximum value in the existing table will be appended</p> required Example <pre><code>from blueno.etl import incremental\nimport polars as pl\n\n# Create sample data\ndata = pl.DataFrame({\"timestamp\": [\"2024-05-24T10:00:00\"], \"value\": [100]})\n\n# Append only records newer than existing data\nincremental(\"path/to/incremental_delta_table\", data, incremental_column=\"timestamp\")\n</code></pre>"},{"location":"api/etl/#ducklake","title":"DuckLake","text":""},{"location":"api/etl/#parquet","title":"Parquet","text":""},{"location":"api/etl/#blueno.etl.load.parquet.write_parquet","title":"write_parquet","text":"<pre><code>write_parquet(\n    uri: str,\n    df: DataFrameType,\n    partition_by: Optional[List[str]] = None,\n) -&gt; None\n</code></pre> <p>Overwrites the entire parquet file or directory (if using <code>partition_by</code>) with the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The file or directory URI to write to. This should be a path if using <code>partition_by</code></p> required <code>df</code> <code>DataFrameType</code> <p>The dataframe to write</p> required <code>partition_by</code> <code>Optional[List[str]]</code> <p>Column(s) to partition by</p> <code>None</code> Example <pre><code>from blueno.etl import write_parquet\nimport polars as pl\n\n# Create sample data with dates\ndata = pl.DataFrame(\n    {\"year\": [2024, 2024, 2024], \"month\": [1, 2, 3], \"value\": [100, 200, 300]}\n)\n\n# Write data partitioned by year and month\nwrite_parquet(uri=\"path/to/parquet\", df=data, partition_by=[\"year\", \"month\"])\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>Custom blueno exceptions used through the library</p>"},{"location":"api/exceptions/#blueno.exceptions.BluenoUserError","title":"BluenoUserError","text":"<p>               Bases: <code>Exception</code></p> <p>An exception for user errors.</p>"},{"location":"api/exceptions/#blueno.exceptions.GenericBluenoError","title":"GenericBluenoError","text":"<p>               Bases: <code>Exception</code></p> <p>Catch-all exception for generic Blueno orchestration errors.</p>"},{"location":"api/exceptions/#blueno.exceptions.DuplicateJobError","title":"DuplicateJobError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when attempting to create a job that already exists.</p>"},{"location":"api/exceptions/#blueno.exceptions.InvalidJobError","title":"InvalidJobError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when attempting to create a job that is invalid.</p>"},{"location":"api/exceptions/#blueno.exceptions.JobNotFoundError","title":"JobNotFoundError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a job is not found.</p>"},{"location":"api/exceptions/#blueno.exceptions.Unreachable","title":"Unreachable","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when supposedly unreachable code is executed (should never happen).</p>"},{"location":"api/utils/","title":"Utilities","text":"<p>Collection of utility functions.</p>"},{"location":"api/utils/#blueno.utils.quote_identifier","title":"quote_identifier","text":"<pre><code>quote_identifier(\n    identifier: str, quote_character: str = '\"'\n) -&gt; str\n</code></pre> <p>Quotes the given identifier by surrounding it with the specified quote character.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The identifier to be quoted.</p> required <code>quote_character</code> <code>str</code> <p>The character to use for quoting. Defaults to '\"'.</p> <code>'\"'</code> <p>Returns:</p> Type Description <code>str</code> <p>The quoted identifier.</p> <p>Example: <pre><code>from blueno.utils import quote_identifier\n\nquote_identifier(\"my_object\")\n'\"my_object\"'\n\nquote_identifier(\"my_object\", \"'\")\n\"'my_object'\"\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.remove_none","title":"remove_none","text":"<pre><code>remove_none(obj: Union[Dict, List]) -&gt; Union[Dict, List]\n</code></pre> <p>Recursively remove None values from dictionaries and lists.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Dict, List]</code> <p>The data structure to clean.</p> required <p>Returns:</p> Type Description <code>Union[Dict, List]</code> <p>A new data structure with None values removed.</p>"},{"location":"api/utils/#blueno.utils.separator_indices","title":"separator_indices","text":"<pre><code>separator_indices(string: str, separator: str) -&gt; list[int]\n</code></pre> <p>Find indices of a separator character in a string, ignoring separators inside quotes.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The input string to search through</p> required <code>separator</code> <code>str</code> <p>The separator character to find</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of indices where the separator character appears outside of quotes</p> <p>Example: <pre><code>from blueno.utils import separator_indices\n\nseparator_indices('a,b,\"c,d\",e', \",\")\n[1, 8]\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.shorten_dict_values","title":"shorten_dict_values","text":"<pre><code>shorten_dict_values(\n    obj: Union[List, Dict], max_length: int = 20\n) -&gt; Union[List, Dict]\n</code></pre> <p>Recursively shorten string values in dictionaries and lists. Useful for printing out data structures in a readable format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[List, Dict]</code> <p>The data structure to shorten.</p> required <code>max_length</code> <code>int</code> <p>The maximum length of string values to shorten.</p> <code>20</code> <p>Returns:</p> Type Description <code>Union[List, Dict]</code> <p>A new data structure with string values shortened.</p>"},{"location":"api/utils/#blueno.utils.get_delta_table_if_exists","title":"get_delta_table_if_exists","text":"<pre><code>get_delta_table_if_exists(\n    table_uri: str,\n) -&gt; DeltaTable | None\n</code></pre> <p>Retrieves a Delta table. Returns None if not exists.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>str</code> <p>The URI of the Delta table.</p> required <p>Returns:</p> Type Description <code>DeltaTable | None</code> <p>The Delta table.</p>"},{"location":"api/utils/#blueno.utils.get_delta_table_or_raise","title":"get_delta_table_or_raise","text":"<pre><code>get_delta_table_or_raise(table_uri: str) -&gt; DeltaTable\n</code></pre> <p>Retrieves a Delta table. Raises exception if not exists.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>str</code> <p>The URI of the Delta table.</p> required <p>Returns:</p> Type Description <code>DeltaTable</code> <p>The Delta table.</p>"},{"location":"api/utils/#blueno.utils.get_last_commit_property","title":"get_last_commit_property","text":"<pre><code>get_last_commit_property(\n    table_or_uri: str | DeltaTable,\n    commit_info_key: str,\n    limit: int = 50,\n) -&gt; str | None\n</code></pre> <p>Retrieves the last modified time of a Delta table. Returns None if table doesn't exist, or if commit info doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>A string URI to a Delta table.</p> required <code>commit_info_key</code> <code>str</code> <p>The key of the info</p> required <code>limit</code> <code>int</code> <p>The maximum log files to check from. Set to <code>None</code> to check entire transaction log. WARNING: This may be costly on tables with many transactions!</p> <code>50</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The value of the key in the Delta table transaction history.</p> <p>Example: ```python notest from blueno.utils import get_last_modified_time</p> <p>last_modified = get_last_commit_info(\"path/to/delta_table\", \"timestamp\") ```</p>"},{"location":"api/utils/#blueno.utils.get_last_modified_time","title":"get_last_modified_time","text":"<pre><code>get_last_modified_time(\n    table_or_uri: str | DeltaTable,\n    operations: list[str],\n    limit: int = 50,\n) -&gt; datetime | None\n</code></pre> <p>Retrieves the last modified time of a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>A string URI to a Delta table.</p> required <code>operations</code> <code>list[str]</code> <p>The operations to search for. Should be a list of one or more of the following:</p> required <code>limit</code> <code>int</code> <p>The maximum log files to check from. Set to <code>None</code> to check entire transaction log. WARNING: This may be costly on tables with many transactions!</p> <code>50</code> <p>Returns:</p> Type Description <code>datetime | None</code> <p>The last modified time of the table, or None if the table does not exist.</p> <p>Example: ```python notest from blueno.utils import get_last_modified_time</p> <p>last_modified = get_last_modified_time(\"path/to/delta_table\", [\"OPTIMIZE\"]) ```</p>"},{"location":"api/utils/#blueno.utils.get_max_column_value","title":"get_max_column_value","text":"<pre><code>get_max_column_value(\n    table_or_uri: str | DeltaTable, column_name: str\n) -&gt; Any\n</code></pre> <p>Retrieves the maximum value of the specified column from a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>A string URI to a Delta table or a DeltaTable instance.</p> required <code>column_name</code> <code>str</code> <p>The name of the column.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The maximum value of the column, or None if the table does not exist.</p> <p>Example: ```python notest from blueno.utils import get_max_column_value</p> <p>max_value = get_max_column_value(\"path/to/delta_table\", \"incremental_id\") ```</p>"},{"location":"api/utils/#blueno.utils.get_min_column_value","title":"get_min_column_value","text":"<pre><code>get_min_column_value(\n    table_or_uri: str | DeltaTable, column_name: str\n) -&gt; Any\n</code></pre> <p>Retrieves the maximum value of the specified column from a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>table_or_uri</code> <code>str | DeltaTable</code> <p>A string URI to a Delta table or a DeltaTable instance.</p> required <code>column_name</code> <code>str</code> <p>The name of the column.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The minimum value of the column, or None if the table does not exist.</p> <p>Example: ```python notest from blueno.utils import get_min_column_value</p> <p>min_value = get_min_column_value(\"path/to/delta_table\", \"incremental_id\") ```</p>"},{"location":"api/utils/#blueno.utils.get_or_create_delta_table","title":"get_or_create_delta_table","text":"<pre><code>get_or_create_delta_table(\n    table_uri: str, schema: Schema\n) -&gt; DeltaTable\n</code></pre> <p>Retrieves a Delta table or creates a new one if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>table_uri</code> <code>str</code> <p>The URI of the Delta table.</p> required <code>schema</code> <code>Schema</code> <p>The Polars or PyArrow schema to create the Delta table with.</p> required <p>Returns:</p> Type Description <code>DeltaTable</code> <p>The Delta table.</p>"},{"location":"api/utils/#blueno.utils.build_merge_predicate","title":"build_merge_predicate","text":"<pre><code>build_merge_predicate(\n    columns: list[str],\n    source_alias: str = \"source\",\n    target_alias: str = \"target\",\n) -&gt; str\n</code></pre> <p>Constructs a SQL merge predicate based on the provided column names.</p> <p>This function generates a string that represents the condition for merging records based on equality of the specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>A list of column names to be used in the merge predicate.</p> required <code>source_alias</code> <code>str</code> <p>An alias for the source</p> <code>'source'</code> <code>target_alias</code> <code>str</code> <p>An alias for the target</p> <code>'target'</code> <p>Returns:</p> Type Description <code>str</code> <p>A SQL string representing the merge predicate.</p> <p>Example: <pre><code>from blueno.utils import build_merge_predicate\n\npredicate = build_merge_predicate(['id', 'name'])\nprint(predicate)\n\"\"\"\n    (target.\"id\" = source.\"id\") AND (target.\"name\" = source.\"name\")\n\"\"\"\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.build_when_matched_update_columns","title":"build_when_matched_update_columns","text":"<pre><code>build_when_matched_update_columns(\n    columns: list[str],\n    source_alias: str = \"source\",\n    target_alias: str = \"target\",\n) -&gt; dict[str, str]\n</code></pre> <p>Constructs a mapping of columns to be updated when a match is found.</p> <p>This function generates a dictionary where the keys are the target column names and the values are the corresponding source column names.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>A list of column names to be used in the update mapping.</p> required <code>source_alias</code> <code>str</code> <p>An alias for the source</p> <code>'source'</code> <code>target_alias</code> <code>str</code> <p>An alias for the target</p> <code>'target'</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary mapping target columns to source columns.</p> <p>Example: <pre><code>from blueno.utils import build_when_matched_update_columns\n\nupdate_columns = build_when_matched_update_columns([\"id\", \"name\"])\nprint(update_columns)\n\n{'target.\"id\"': 'source.\"id\"', 'target.\"name\"': 'source.\"name\"'}\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.build_when_matched_update_predicate","title":"build_when_matched_update_predicate","text":"<pre><code>build_when_matched_update_predicate(\n    existing_columns: list[str],\n    new_columns: list[str] | None = None,\n    source_alias: str = \"source\",\n    target_alias: str = \"target\",\n) -&gt; str\n</code></pre> <p>Constructs a SQL predicate for when matched update conditions.</p> <p>This function generates a string that represents the conditions for updating records when a match is found based on the specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>existing_columns</code> <code>list[str]</code> <p>A list of column names to be used in the update predicate. These columns must existing both source and target dataframe.</p> required <code>new_columns</code> <code>list[str] | None</code> <p>A list of columns only existing in the source.</p> <code>None</code> <code>source_alias</code> <code>str</code> <p>An alias for the source</p> <code>'source'</code> <code>target_alias</code> <code>str</code> <p>An alias for the target</p> <code>'target'</code> <p>Returns:</p> Type Description <code>str</code> <p>A SQL string representing the when matched update predicate.</p> <p>Example: <pre><code>from blueno.utils import build_when_matched_update_predicate\n\nupdate_predicate = build_when_matched_update_predicate(['id', 'status'])\nprint(update_predicate)\n\"\"\"\n    (\n        (target.\"id\" != source.\"id\")\n        OR (target.\"id\" IS NULL AND source.\"id\" IS NOT NULL)\n        OR (target.\"id\" IS NOT NULL AND source.\"id\" IS NULL)\n    ) OR ...\n\"\"\"\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.character_translation","title":"character_translation","text":"<pre><code>character_translation(\n    text: str, translation_map: Dict[str, str]\n) -&gt; str\n</code></pre> <p>Translate characters in a string using a translation map.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to translate.</p> required <code>translation_map</code> <code>Dict[str, str]</code> <p>A dictionary mapping characters to their replacements.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The translated string.</p> <p>Example: <pre><code>from blueno.utils import character_translation\n\ncharacter_translation(\"Profit&amp;Loss\", {\"&amp;\": \"_and\"})\n\"Profit_and_Loss\"\n</code></pre></p>"},{"location":"api/utils/#blueno.utils.to_snake_case","title":"to_snake_case","text":"<pre><code>to_snake_case(text: str) -&gt; str\n</code></pre> <p>Convert a string to snake case.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The string to convert to snake case. Can be converted from PascalCase, camelCase, kebab-case, or mixed case. Non-alphanumeric characters are converted to underscores.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string in snake case.</p> <p>Example: <pre><code>from blueno.utils import to_snake_case\n\nto_snake_case(\"CustomerID\")\n\"customer_id\"\n</code></pre></p>"},{"location":"features/","title":"Features","text":"<p>Let's look at what makes blueno special! Here are the key features that will help you build declarative data pipelines:</p>"},{"location":"features/#local-development-first","title":"Local Development First","text":"<p>Why wait for the cloud when you can develop locally? \ud83d\udcbb</p> <p>blueno lets you:</p> <ul> <li>Write and test your pipelines on your local machine</li> <li>Use the same code for local and cloud environments</li> <li>Connect to your blob storage when needed</li> </ul>"},{"location":"features/#deploy-anywhere","title":"Deploy anywhere","text":"<p>There are no external dependencies on where you can run blueno. You can run it on any compute which has Python installed and bring your own blob storage. </p> <p>You can for example run on AWS Lambda, Azure Functions, Container Apps, Google Collab Notebook or some virtual machine with underlying data storage such as AWS S3 or Azure Data Lake Storage.</p> <p>And of course you can also run on your local machine with your local file system as the storage layer.</p>"},{"location":"features/#serverless","title":"Serverless","text":"<p>Blueno is first and foremost ment to be run in a serverless manner. </p> <p>There are no services required to track state. Functional state is stored on the tables themselves (i.e. watermarks for incremental loads), and observability metrics are emitted through logs. </p>"},{"location":"features/#etl-helper-functions","title":"ETL Helper Functions","text":"<p>blueno comes with batteries included for all your ETL needs:</p>"},{"location":"features/#extract","title":"Extract","text":"<ul> <li>Blueno supports reading from delta tables or parquet files</li> <li>Automatic authentication with Azure Data Lake or Fabric OneLake</li> </ul>"},{"location":"features/#transform","title":"Transform","text":"<p>Common data transformations out of the box:</p> <ul> <li>Add audit columns</li> <li>Normalize column names</li> <li>Remove duplicates</li> <li>And much more!</li> </ul>"},{"location":"features/#load","title":"Load","text":"<p>Multiple load strategies:</p> <ul> <li>Upsert (merge)</li> <li>Overwrite</li> <li>Incremental</li> <li>Append</li> <li>Replace range (overwrite range)</li> <li>SCD Type 2</li> </ul>"},{"location":"features/#blueprints","title":"Blueprints","text":"<p>Blueprints are the heart of blueno. They let you:</p> <ul> <li>Write declarative data pipelines</li> <li>Focus on business logic, not boilerplate</li> <li>Automatically handle dependencies</li> <li>Automatic schema validation</li> <li>Visualize your data pipeline as a DAG</li> <li>Run your pipelines locally or in the cloud</li> </ul>"},{"location":"features/#extendable-through-plugins","title":"Extendable through plugins","text":"<p>blueno is has an interface for creating plugins. You can add custom plugins for custom transformations used as post-transformations or custom load strategies.</p>"},{"location":"features/#microsoft-fabric","title":"Microsoft Fabric","text":"<p>While blueno works anywhere, it has special support for Microsoft Fabric:</p> <ul> <li>Run your blueno pipelines directly in a Microsoft Fabric notebook</li> <li>Authenticate automatically with OneLake</li> <li>Use local development workflow with cloud deployment</li> <li>Keep your code clean and platform-agnostic</li> </ul>"},{"location":"learn/examples/","title":"Examples","text":"<p>The Blueno GitHub repository contain various examples. </p>"},{"location":"learn/examples/#jaffle-shop","title":"Jaffle shop","text":"<p>You can see how an simple implementation of the jaffle shop would look in blueno:</p> <ul> <li>https://github.com/mrjsj/blueno/tree/main/examples/jaffle_shop</li> </ul>"},{"location":"learn/examples/#contoso","title":"Contoso","text":"<p>You can see how an implementation of Microsoft Contoso (WIP):</p> <ul> <li>https://github.com/mrjsj/blueno/tree/main/examples/contoso</li> </ul>"},{"location":"learn/quick-start/","title":"Quick start","text":"<p>If you just want to get started, you can download the download an example and run it.</p>"},{"location":"learn/quick-start/#installation","title":"Installation","text":"<p>First install the tool:</p> <code>pip</code><code>uv</code> <pre><code>$ pip install blueno\n---&gt; 100%\nSuccessfully installed blueno\n</code></pre> <pre><code>$ uv add blueno\n---&gt; 100%\nSuccessfully installed blueno\n</code></pre>"},{"location":"learn/quick-start/#create-a-project","title":"Create a project","text":"<p>Copy the example and save it to a folder, e.g. <code>blueprints/example.py</code>.</p> <p>Using the medallion architecture the example consists of:</p> <ul> <li>three mocked bronze source tables</li> <li>three silver tables</li> <li>and a single gold table.</li> </ul> <pre><code>import random\nimport time\n\nimport polars as pl\n\nfrom blueno import Blueprint, DataFrameType\n\nRAND_SIZE = 10\n\n\n@Blueprint.register(table_uri=\"lakehouse/bronze/product\", format=\"delta\")\ndef bronze_product() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"product_id\": [1, 2, 3],\n            \"product_name\": [\"ball\", \"bat\", \"tent\"],\n            \"price\": [4.99, 9.99, 29.99],\n        }\n    )\n\n    time.sleep(random.random() * RAND_SIZE)\n\n    return df\n\n\n@Blueprint.register(table_uri=\"lakehouse/bronze/transaction\", format=\"delta\")\ndef bronze_transaction() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"product_id\": [3, 2, 1, 1],\n            \"transaction_date\": [\"2025-01-01\", \"2025-01-02\", \"2025-01-03\", \"2025-01-04\"],\n            \"quantity\": [2, 1, 1, 0],\n        }\n    )\n    time.sleep(random.random() * RAND_SIZE)\n\n    return df\n\n\n@Blueprint.register(\n    table_uri=\"lakehouse/silver/product\",\n    format=\"delta\",    \n    primary_keys=[\"product_id\"],\n)\ndef silver_product(self: Blueprint, bronze_product: DataFrameType) -&gt; DataFrameType:\n    df = bronze_product.unique(subset=self.primary_keys)\n    time.sleep(random.random() * RAND_SIZE)\n\n    return df\n\n\n@Blueprint.register(\n    table_uri=\"lakehouse/silver/transaction\",\n    format=\"delta\",\n    primary_keys=[\"product_id\"],\n)\ndef silver_transaction(bronze_transaction: DataFrameType) -&gt; DataFrameType:\n    df = bronze_transaction.filter(pl.col(\"quantity\") &gt; 0)\n    time.sleep(random.random() * RAND_SIZE)\n\n    return df\n\n\n@Blueprint.register(\n    table_uri=\"lakehouse/gold/sales_metric\",\n    format=\"delta\",\n    write_mode=\"incremental\",\n    incremental_column=\"transaction_date\",\n)\ndef gold_sales_metric(\n    silver_transaction: DataFrameType,\n    silver_product: DataFrameType,\n) -&gt; DataFrameType:\n    df = (\n        silver_transaction\n        .join(silver_product, on=\"product_id\", how=\"left\")\n        .group_by(\n            \"transaction_date\",\n            \"product_id\",\n        )\n        .agg(\n            pl.sum(\"quantity\").alias(\"total_quantity\"),\n            (pl.col(\"quantity\") * pl.col(\"price\")).sum().alias(\"total_sales\"),\n        )\n    )\n    time.sleep(random.random() * RAND_SIZE)\n\n    return df\n</code></pre>"},{"location":"learn/quick-start/#preview-a-blueprint","title":"Preview a blueprint","text":"<p>Before materializing the tables, or when working on transformations, it's a good idea to preview a transformation first.</p> <p>For example, we can preview the <code>silver_product</code> transformation</p> <pre><code>blueno preview --project-dir ./blueprints --transformation-name silver_product\n</code></pre> <p>This will show the dataframe printed to the terminal.</p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 product_id \u2506 product_name \u2506 price \u2502\n\u2502 ---        \u2506 ---          \u2506 ---   \u2502\n\u2502 i64        \u2506 str          \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3          \u2506 tent         \u2506 29.99 \u2502\n\u2502 2          \u2506 bat          \u2506 9.99  \u2502\n\u2502 1          \u2506 ball         \u2506 4.99  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"learn/quick-start/#run-the-blueprints","title":"Run the blueprints","text":"<p>Now that we validated the transformation outputs the correct data, we can run and materialize all the blueprints.</p> <p>Run using one of the options below by pointing the project directory to the folder you saved the <code>example.py</code> to.</p> CLIPython <pre><code>blueno run --project-dir ./blueprints --concurrency 2\n</code></pre> <p>Create another python file, i.e. <code>main.py</code>, and run it.</p> <pre><code>from blueno.cli import run\n\nrun(project_dir=\"./blueprints\", concurrency=2)\n</code></pre> <p>You should see a table with the run status of the blueprints in the DAG similar to the one below.</p> <p></p>"},{"location":"learn/quick-start/#result","title":"Result","text":"<p>Once complete, you can navigate to the created lakehouse folder and discover what the was created.</p> <p>The final folder structure should look something like this</p> <pre><code>.\n\u251c\u2500\u2500 blueno.log\n\u251c\u2500\u2500 blueprints\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 example.py\n\u251c\u2500\u2500 lakehouse\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bronze\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 product\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00001-00885a2d-23b9-46cb-9d78-fd0e73200e00-c000.snappy.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 transaction\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 part-00001-13268cd8-b475-4d54-a52f-37d86c1858e2-c000.snappy.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 gold\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 sales_metric\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 part-00001-0e8a37a1-63c0-4a43-abc8-748518ad7463-c000.snappy.parquet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 silver\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 product\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00001-aa5dba8e-426a-438e-b5a0-7e1c05208b31-c000.snappy.parquet\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 transaction\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 _delta_log\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u251c\u2500\u2500 00000000000000000000.json\n\u2502\u00a0\u00a0         \u2502\u00a0\u00a0 \u2514\u2500\u2500 00000000000000000001.json\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 part-00001-aa0a9af5-860d-4c10-8fad-362eff6c8415-c000.snappy.parquet\n\u2514\u2500\u2500 main.py\n</code></pre>"},{"location":"learn/blueprints/custom_jobs/","title":"Custom jobs","text":"<p>If the <code>blueprint</code> and <code>task</code> don't fit you needs, you can create your own implementation of the <code>BaseJob</code>.</p> <p>It simply needs to inherit from the <code>blueno.orchetration.job.BaseJob</code> class and implement the <code>run</code> method.</p> <p>In addition, a decorator should be implemented. You can see the simple implementation of <code>Task</code> for inspiration.</p>"},{"location":"learn/blueprints/custom_jobs/#example","title":"Example","text":"<p>We want to create a custom job which can calls a webhook specified in the decorator. The payload is set as the output of the decorated function.</p> <pre><code>from blueno.orchestration.job import BaseJob, job_registry, track_step\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport requests\n\n@dataclass(kw_only=True)\nclass WebhookJob(BaseJob):\n    \"\"\"Class for the webhook_job decorator.\"\"\"\n\n    webhook_url: str\n\n    @track_step\n    def run(self):\n        \"\"\"Running the webhook job.\"\"\"\n        msg = self._transform_fn(*self.depends_on)\n        payload = {\n            \"msg\": msg\n        }\n        requests.post(self.webhook_url, data=payload)\n\n\n\ndef webhook_job(\n    _func=None,\n    *,\n    name: Optional[str] = None,\n    priority: int = 100,\n    webhook_url: str\n):\n    \"\"\"Create a definition for webhook_job\"\"\"\n\n    def decorator(func: types.FunctionType):\n        _name = name or func.__name__\n\n        webhook_job = WebhookJob(\n            name=_name,\n            _transform_fn=func,\n            webhook_url=webhook_url,\n            priority=priority,\n        )\n\n        webhook_job._register(job_registry)\n\n        return task\n\n    # If used as @webhook_job\n    if _func is not None and callable(_func):\n        return decorator(_func)\n\n    # If used as @webhook_job(...)\n    return decorator\n</code></pre> <p>Now we can use the <code>webhook_job</code> decorator:</p> <pre><code>@webhook_job(\n    webhook_url=\"https://some-webhook-uri.com/webhook\"\n)\ndef send_on_complete() -&gt; str:\n\n    msg = \"The data pipeline completed successfully\"\n\n    return msg\n</code></pre>"},{"location":"learn/blueprints/introduction/","title":"Introduction","text":"<p>A blueprint is a Python function decorator to declaratively design a table/entity. It is inspired by dbt and SQLMesh \"models\". However, instead of writing in SQL with jinja-like templates, you write in pure Python - there's no \"compilation\" step.</p> <p>When a function is decorated with the <code>Blueprint.register</code> decorator, it is automatically registered in a <code>JobRegistry</code> which is used for creating the Directed Acyclic Graph (DAG) for running the blueprints in right order, and with highest possible concurrency.</p>"},{"location":"learn/blueprints/introduction/#simple-example","title":"Simple example","text":"<p>A simple example is a \"source\" blueprint. It has no dependencies and can thus be executed as the first blueprint in the DAG. The example just returns a dataframe, however, it could be any source, e.g. reading from external storage, a REST API or a database, however it must return a dataframe.</p> <pre><code>from blueno import blueprint, Blueprint, DataFrameType\n\n\n@Blueprint.register()\ndef bronze_products() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"a\": [1, 2, 3],\n            \"b\": [\"foo\", \"bar\", \"baz\"],\n        }\n    )\n\n    return df\n</code></pre> <p>The <code>blueprint</code> decorator has many inputs. You can see a full list of inputs at the Blueprint API Reference.</p> <p>One parameter is <code>table_uri</code> which defines where to store this dataframe as a delta table. If not specified, the blueprint will not be stored in a table, however it will still be available as an ephemeral dataframe (much like a temporary table) for downstream blueprints.</p> <p>The <code>blueprint</code> can now be run with the command.</p> <pre><code>blueno run --project-path path/to/blueprints\n</code></pre>"},{"location":"learn/blueprints/introduction/#another-example","title":"Another example","text":"<p>The above example is very barebones, and does not utilize the full potential of blueprints. Let's create a more advanced example - a derived table of the <code>bronze_product</code> blueprint presented above.</p> <p>The product blueprint now has proper columns; <code>product_id</code> and <code>product_name</code>. We'll also introduct a customer blueprint, and a sales blueprint for our sources.</p> <p>In addition, the blueprint decorator is now supplied with the <code>table_uri</code> parameter, so that they are stored in delta tables when run.</p> <pre><code>from blueno import Blueprint, DataFrameType\n\n@Blueprint.register(\n    table_uri=\"lakehouse/bronze/products\",\n    format=\"delta\",    \n)\ndef bronze_products() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"product_id\": [1, 2, 3],\n            \"product_name\": [\"ball\", \"bat\", \"tent\"],\n            \"price\": [4.99, 9.99, 29.99]\n        }\n    )\n\n    return df\n\n@Blueprint.register(\n    table_uri=\"lakehouse/bronze/customers\",\n    format=\"delta\",    \n)\ndef bronze_customers() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"customer_id\": [\"CUST01\", \"CUST02\", \"CUST03\"],\n            \"customer_name\": [\"Mary\", \"Bob\", \"Alice\"],\n        }\n    )\n\n    return df\n\n@Blueprint.register(\n    table_uri=\"lakehouse/bronze/sales\",\n    format=\"delta\",    \n)\ndef bronze_sales() -&gt; DataFrameType:\n    df = pl.DataFrame(\n        {\n            \"customer_id\": [\"CUST01\", \"CUST02\", \"CUST03\"],\n            \"product_id\": [3, 2, 1],\n            \"transaction_date\": [\"2025-01-01\", \"2025-01-02\", \"2025-01-03\"],\n            \"quantity\": [2, 1, 1],\n        }\n    )\n\n    return df\n</code></pre> <p>If we now run the command with the <code>--show-dag</code> flag.</p> <pre><code>blueno run path/to/blueprints --show-dag\n</code></pre> <p>We can see the three blueprints. However, since there are no dependencies, the DAG is quite uninteresting.</p> <p></p> <p>So lets add some silver blueprints. The silver layer is usually cleansed data. In this example, we ensure there are no duplicates in the products table and in the customers table.</p> <p>In addition, transactions with a <code>quantity</code> of zero are removed.</p> <pre><code>from blueno import Blueprint\nfrom blueno.types import DataFrameType\n\n@Blueprint.register(\n    table_uri=\"lakehouse/silver/products\",\n    format=\"delta\",    \n    primary_keys=[\"product_id\"],\n)\ndef silver_products(self: Blueprint, bronze_products: DataFrameType) -&gt; DataFrameType:\n    df = bronze_products.unique(subset=self.primary_keys)\n\n    return df\n\n\n@Blueprint.register(\n    table_uri=\"lakehouse/silver/customers\",\n    format=\"delta\",    \n    primary_keys=[\"customer_id\"],\n)\ndef silver_customers(self: Blueprint, bronze_customers: DataFrameType) -&gt; DataFrameType:\n    df = bronze_customers.unique(subset=self.primary_keys)\n\n    return df\n\n\n@Blueprint.register(\n    table_uri=\"lakehouse/silver/transactions\",\n    format=\"delta\",\n    primary_keys=[\"product_id\"],\n)\ndef silver_transactions(bronze_transactions: DataFrameType) -&gt; DataFrameType:\n    df = bronze_transactions.filter(pl.col(\"quantity\") &gt; 0)\n\n    return df\n</code></pre> <p>Let's look at the DAG:</p> <p></p> <p>As expected, we can see the that the bronze tables are upstream dependencies of the silver tables. How is this achieved?</p> <p>It is achieved by adding a parameter to the function signature of the blueprint. For example, we added the <code>bronze_transactions: DataFrameType</code> to the <code>silver_transactions</code> function signature. This parameter name <code>bronze_transactions</code> exactly matches the function name of the <code>bronze_transactions</code> blueprint. Then we can automatically infer this dependency.</p> <p>In the <code>silver_products</code> and <code>silver_customers</code> blueprints you can also see the parameter <code>self: Blueprint</code>. This can optionally be added if you need to access to some of the blueprints parameters, i.e. <code>table_uri</code> or <code>primary_keys</code>. In this example, we need to know our primary keys to deduplicate.</p> <p>Lastly, we will add the metrics table in the gold layer, which combines the three silver tables:</p> <pre><code>from blueno import Blueprint\nfrom blueno.types import DataFrameType\n\n@Blueprint.register(\n    table_uri=\"lakehouse/gold/sales_metrics\",\n    format=\"delta\",    \n    write_mode=\"incremental\",\n    incremental_column=\"transaction_date\",\n)\ndef gold_sales_metrics(\n    silver_transactions: DataFrameType,\n    silver_customers: DataFrameType,\n    silver_products: DataFrameType\n) -&gt; DataFrameType:\n\n    df = (\n        silver_transactions\n        .join(silver_customers, on=\"customer_id\", how=\"inner\")\n        .join(silver_products, on=\"product_id\", how=\"inner\")\n        .group_by(\n            \"transaction_date\",\n            \"customer_id\",\n            \"product_id\",\n        )\n        .agg(\n            pl.sum(\"quantity\").alias(\"total_quantity\"),\n            (pl.col(\"quantity\") * pl.col(\"price\")).sum().alias(\"total_sales\"),\n        )\n    )\n\n    return df\n</code></pre> <p>All three silver blueprints are added as dependencies and are inner joined to ensure we only show sales metrics for valid products and valid customers. In addition, the total sales metric is calculated by multiplying the quantity and price.</p> <p>In the <code>blueprint</code> decorator some new parameters were introduced. Namely <code>write_mode</code> and <code>incremental_column</code>. The <code>write_mode</code> determines how the resulting dataframe is written to the destination. The <code>incremental</code> write mode filters the dataframe on the <code>incremental_column</code> by the max value of the <code>incremental_column</code> in the target table. After it is filtered, the resulting dataframe is appended to the target table. If not supplied, the <code>write_mode</code> defaults to <code>overwrite</code>. </p> <p>You can see a complete list of supported write modes in the Blueprint API Reference.</p> <p>Now we can run the blueprints again, however this time I supply the command with <code>--select</code>. This ensures we only run the <code>gold_sales_metrics</code> blueprint, because I know we already ran its upstream dependencies, so there is no need to re-run them.</p> <pre><code>blueno run --project-path path/to/blueprints\n           --select gold_sales_metrics \\\n           --display-mode log \\\n           --log-level INFO \\\n           --show-dag\n</code></pre> <p>Tip</p> <p>You can for example also use <code>--select +gold_sales_metrics</code> (prefixed and/or suffixed by one or more <code>+</code>'s).</p> <p>The number of <code>+</code>'s will denote the number of levels of upstream and downstream dependencies to also run.</p> <p>I.e. <code>+gold_sales_metrics</code> will select the <code>gold_sales_metrics</code> job and its direct upstream dependencies (parents). <code>++silver_product</code> will select the <code>silver_product</code> job and two generations of upstream dependencies (parents + grandparents).</p> <p>You can also be more specific such as <code>--select bronze_products+ silver_customers</code>. This will run <code>bronze_products</code> and its direct downstream depencies (<code>silver_products</code>); and <code>silver_customers</code></p> <p>Note</p> <p>You need <code>graphviz</code>installed to show the DAG. If you do not have it installed, you can exclude the <code>--show-dag</code> option.</p> <p>We can see in the console output, that only the <code>gold_sales_metrics</code> was run.</p> <pre><code>2025-05-18 18:10:51 - blueno - INFO - Starting blueprint execution 1 tasks at a time\n2025-05-18 18:10:51 - blueno - INFO - Running: gold_sales_metrics\n2025-05-18 18:10:51 - blueno - INFO - Finished: gold_sales_metrics\n</code></pre> <p>The resulting DAG is:</p> <p></p> <p>And we can see the resulting sales metrics table by printing to console.</p> <pre><code>import polars as pl\n\ndf = pl.read_delta(\"lakehouse/gold/sales_metrics\")\nprint(df)\n</code></pre> transaction_date customer_id product_id total_quantity total_sales str str i64 i64 f64 2025-01-03 CUST03 1 1 4.99 2025-01-02 CUST02 2 1 9.99 2025-01-01 CUST01 3 2 59.98"},{"location":"learn/blueprints/introduction/#extending-the-blueprint-class","title":"Extending the <code>Blueprint</code> class","text":"<p>Sometimes, the built-in write methods or post-processing steps in <code>Blueprint</code> might not be enough for your needs. In these cases, you can create your own class that inherits from <code>Blueprint</code> and override specific methods or properties.</p> <p>For example, let\u2019s say you want to:</p> <ul> <li>Add a post-processing step that extracts the date from a timestamp column.</li> <li>Add a custom write method that overwrites only specific partitions in your table.</li> </ul> <p>To do this, you\u2019ll need to override the following properties or methods:</p> <ul> <li><code>_extend_post_transforms</code> to add your custom post-processing.</li> <li><code>_extend_write_modes</code> to add your custom write method.</li> <li>Optionally, <code>_extend_input_validations</code> to add extra input checks.</li> </ul>"},{"location":"learn/blueprints/introduction/#add-a-custom-post-transformation","title":"Add a custom post transformation","text":"<p>First, create a new class that inherits from <code>Blueprint</code>. Add a method to extract the date from a timestamp column, and register it in <code>_extend_post_transforms</code>:</p> <pre><code>from blueno import Blueprint\nfrom dataclasses import dataclass\nimport polars as pl\nfrom typing import Optional, Dict, Callable\n\n@dataclass(kw_only=True)\nclass CustomBlueprint(Blueprint):\n    date_partition_column: Optional[str] = None\n\n    def _add_date_partition(self) -&gt; pl.DataFrame:\n        self._dataframe = self._dataframe.with_columns(\n            pl.col(self.date_partition_column).dt.date().alias(\"date\"),\n        )\n\n    @property\n    def _extend_post_transforms(self) -&gt; Dict[str, Callable]:\n        return {\n            \"add_date_partition\": self._add_date_partition\n        }\n</code></pre>"},{"location":"learn/blueprints/introduction/#use-your-custom-blueprint","title":"Use your custom blueprint","text":"<p>Now you can use your new class and specify the custom post-transform:</p> <pre><code>from datetime import datetime, timezone\n\n@CustomBlueprint.register(\n    table_uri=\"/tmp/my_deltatable\",\n    format=\"delta\",\n    date_partition_column=\"ts\",\n    post_transforms=[\"add_date_partition\"]\n)\ndef my_transformation():\n    df = pl.DataFrame({\n        \"a\": [1],\n        \"ts\": [datetime.now(timezone.utc)]\n    })\n    return df\n</code></pre>"},{"location":"learn/blueprints/introduction/#add-custom-input-validation-optional-but-recommended","title":"Add custom input validation (optional, but recommended)","text":"<p>To make sure users don\u2019t forget to set <code>date_partition_column</code> when using your post-transform, you can add a validation rule:</p> <pre><code>from typing import List, Tuple\n\n@dataclass(kw_only=True)\nclass CustomBlueprint(Blueprint):\n    # ...existing code...\n\n    @property\n    def _extend_input_validations(self) -&gt; List[Tuple[bool, str]]:\n        return [\n            (\n                \"add_date_partition\" in self._post_transforms and not isinstance(self.date_partition_column, str),\n                \"date_partition_column must be provided when add_date_partition post_transform is set\"\n            )\n        ]\n</code></pre>"},{"location":"learn/blueprints/introduction/#add-a-custom-write-mode","title":"Add a custom write mode","text":"<p>You can also add a write method that only overwrites the partitions present in your dataframe:</p> <pre><code>from deltalake import write_deltalake\n\n@dataclass(kw_only=True)\nclass CustomBlueprint(Blueprint):\n    # ...existing code...\n\n    def _write_mode_overwrite_partition(self) -&gt; None:\n        partitions = self._dataframe.select(\"date\").unique().to_dict(as_series=False).get(\"date\", [])\n        partitions_str = [partition.strftime(\"%Y-%m-%d\") for partition in partitions]\n        partition_predicate = f\"\"\"date in ('{\"','\".join(partitions_str)}')\"\"\" if partitions_str else None\n\n        write_deltalake(\n            table_or_uri=self.table_uri,\n            data=self._dataframe.to_arrow(),\n            partition_by=[\"date\"],\n            mode=\"overwrite\",\n            predicate=partition_predicate,\n        )\n\n    @property\n    def _extend_write_modes(self) -&gt; Dict[str, Callable]:\n        return {\n            \"overwrite_partition\": self._write_mode_overwrite_partition\n        }\n</code></pre> <p>For a complete, working example, see: test_custom_blueprint</p>"},{"location":"learn/blueprints/tasks/","title":"Tasks","text":"<p>If you need something orchestrated in your DAG which shouldn't be expressed a blueprint, then you can use a <code>task</code>.</p> <p>A task is just any arbitrary code which can have any number (or zero) upstream or downstream dependencies. </p> <p>This could for example be a Slack notification or a web hook request, or some custom logging.</p> <pre><code>from blueno import Blueprint, Task\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@Task.register()\ndef notify_start() -&gt; None:\n    logger.info(\"Blueno data pipeline started.\")\n\n@Blueprint.register()\ndef some_blueprint_definition(notify_start):\n    ...\n\n@Task.register()\ndef notify_end(some_blueprint_definition) -&gt; None:\n    logger.info(\"Blueno data pipeline ended.\")\n</code></pre> <p>This will create the following DAG:</p> <pre><code>flowchart TD\n    notify_start[notify_start] --&gt; some_blueprint_definition[some_blueprint_definition]\n    some_blueprint_definition --&gt; notify_end[notify_end]</code></pre>"}]}